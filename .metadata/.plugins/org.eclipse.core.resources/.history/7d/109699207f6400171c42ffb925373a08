package hadoop;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;

import org.apache.hadoop.fs.Path;

import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Job;

import org.apache.hadoop.mapreduce.Mapper;

import org.apache.hadoop.mapreduce.Reducer;

import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import org.apache.hadoop.util.GenericOptionsParser;

public class driverConfig {
	public static void main(String[] args) throws Exception {

		Configuration conf = new Configuration();

		// 这句话很关键

		conf.set("mapred.job.tracker", "192.168.1.2:9001");

		String[] ioArgs = new String[] { "dedup_in", "dedup_out" };

		String[] otherArgs = new GenericOptionsParser(conf, ioArgs).getRemainingArgs();

		if (otherArgs.length != 2) {

			System.err.println("Usage: Data Deduplication <in> <out>");

			System.exit(2);

		}

		Job job = new Job(conf, "Data Deduplication");

		job.setJarByClass(driverConfig.class);

		// 设置Map、Combine和Reduce处理类

		job.setMapperClass(vMap.class);

		job.setCombinerClass(vReduce.class);

		job.setReducerClass(vReduce.class);

		// 设置输出类型

		job.setOutputKeyClass(Text.class);

		job.setOutputValueClass(Text.class);

		// 设置输入和输出目录

		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));

		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));

		System.exit(job.waitForCompletion(true) ? 0 : 1);

	}
}

// map将输入中的value复制到输出数据的key上，并直接输出
